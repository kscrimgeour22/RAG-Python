{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17lwAusUoqSb_MWIL8BbWtz5QH-ED3psr","timestamp":1739489871759}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","\n","!pip install faiss-cpu fitz requests numpy langchain openai azure-storage-blob azure-search-documents tiktoken"],"metadata":{"id":"yvyomubv2L9y","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade pymupdf"],"metadata":{"collapsed":true,"id":"wXU3-YDL4O80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install langchain-community\n"],"metadata":{"collapsed":true,"id":"MDqc41xO4gfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import neccessary libraries for creating AI assistant with correct vector store.\n","\n","import os\n","\n","import json\n","import zipfile\n","import fitz  # PyMuPDF\n","import faiss\n","import numpy as np\n","import requests\n","import logging\n","from typing import List, Any\n","from azure.storage.blob import BlobServiceClient\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from azure.core.credentials import AzureKeyCredential\n","from azure.search.documents.indexes import SearchIndexClient\n","from azure.search.documents import SearchClient\n","from azure.search.documents.indexes.models import (\n","    SearchIndex,\n","    SearchField,\n","    SearchFieldDataType,\n","    VectorSearchProfile, #I added this module.\n","    HnswAlgorithmConfiguration,\n","    VectorSearchAlgorithmKind,\n","    VectorSearchAlgorithmMetric,\n","    VectorSearch,\n",")\n","from openai import AzureOpenAI"],"metadata":{"id":"jbQ38TyPcFSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Click each of the cells in order. If there is an issue re-click the cell and run again. Cell 5 contains the codes that call each of the functions. You run the functions 1-4 to load them in to memory then run five to call all the functions and create the RAG on Azure.  Watch your directory names and locations as I set it up for a directory on Colab.\n"],"metadata":{"id":"xbvOHOMcuGjw"}},{"cell_type":"code","source":["# 1️⃣ Setup & Configuration\n","\n","logging.basicConfig(level=logging.INFO)\n","\n","AZURE_STORAGE_CONNECTION_STRING = \"DefaultEndpointsProtocol=https;AccountName=xxxxxxxx;AccountKey=xxxxxx==;EndpointSuffix=core.windows.net\"  #Settings + networking > Access keys in your storage account's menu\n","CONTAINER_NAME = \"vector-database\" #use any name you want\n","\n","config_data = {\n","    \"AZURE_OPENAI_KEY\": \"XXXXXXXXX\", # find in access key or in view code in assistant\n","    \"AZURE_OPENAI_ENDPOINT\": \"https://xxxxxx.openai.azure.com/\",\n","    \"AZURE_OPENAI_APIVERSION\": \"2024-05-01-preview\",\n","    \"DEPLOYMENT_NAME\": \"XXXXX-gpt-4o-mini\",\n","    \"MODEL_NAME\": \"gpt-4o-mini\"\n","}\n","\n","SEARCH_SERVICE_NAME = \"your-data-search\"   # any name you want\n","SEARCH_API_KEY = \"xxxxxxxxxxxxxx\"  # find in key under storage\n","INDEX_NAME = \"yourblob-index\" # use any name you want\n","ZIP_FILE_PATH = \"/content/file-name.zip\" # put the file name and path here - Use WinRarZip as it works best, not Mac-compress\n","pdf_dir = \"/Data\"\n","FAISS_INDEX_PATH = \"/content/\""],"metadata":{"id":"zW2xtGZP2lvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2️⃣ File Preparation\n","\n","def extract_zip(zip_path: str, extract_path: str) -> None:\n","    \"\"\"Extract ZIP file to specified path.\"\"\"\n","    try:\n","        # Ensure the extraction directory exists\n","        os.makedirs(extract_path, exist_ok=True)\n","\n","        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","            zip_ref.extractall(extract_path)\n","        logging.info(f\"✅ Successfully extracted ZIP file to {extract_path}\")\n","    except zipfile.BadZipFile:\n","        logging.error(f\"❌ File is not a valid ZIP file: {zip_path}\")\n","        raise\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to extract ZIP file: {str(e)}\")\n","        raise\n","\n","def extract_text_from_pdfs(pdf_dir: str) -> List[str]:\n","    \"\"\"Extract text from all PDFs in directory.\"\"\"\n","    texts = []\n","    try:\n","        for filename in os.listdir(pdf_dir):\n","            if filename.lower().endswith('.pdf'):\n","                pdf_path = os.path.join(pdf_dir, filename)\n","                try:\n","                    with fitz.open(pdf_path) as doc:\n","                        text = \"\"\n","                        for page in doc:\n","                            text += page.get_text()\n","                        texts.append(text)\n","                    logging.info(f\"✅ Successfully extracted text from {filename}\")\n","                except Exception as e:\n","                    logging.error(f\"❌ Failed to process PDF {filename}: {str(e)}\")\n","        return texts\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to read PDF directory: {str(e)}\")\n","        raise\n","\n","def chunk_text(texts: List[str], chunk_size: int = 1000) -> List[str]:\n","    \"\"\"Split texts into smaller chunks.\"\"\"\n","    chunks = []\n","    try:\n","        for text in texts:\n","            # Simple chunking by character count\n","            for i in range(0, len(text), chunk_size):\n","                chunk = text[i:i + chunk_size]\n","                if chunk.strip():  # Only add non-empty chunks\n","                    chunks.append(chunk)\n","        logging.info(f\"✅ Created {len(chunks)} text chunks\")\n","        return chunks\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to chunk text: {str(e)}\")\n","        raise\n","\n","def create_faiss_index(texts: List[str], dimension: int = 1536) -> str:\n","    \"\"\"Create FAISS index from text chunks.\"\"\"\n","    try:\n","        # Initialize FAISS index\n","        index = faiss.IndexFlatL2(dimension)\n","\n","        # Convert texts to vectors (placeholder - you'll need to implement actual embedding)\n","        vectors = np.random.rand(len(texts), dimension).astype('float32')\n","\n","        # Add vectors to index\n","        index.add(vectors)\n","\n","        # Save index\n","        faiss_path = \"faiss_index.bin\"\n","        faiss.write_index(index, faiss_path)\n","\n","        logging.info(f\"✅ Created FAISS index with {len(texts)} vectors\")\n","        return faiss_path\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to create FAISS index: {str(e)}\")\n","        raise\n","\n","def upload_faiss_to_azure(index_path: str) -> None:\n","    \"\"\"Upload FAISS index to Azure Storage.\"\"\"\n","    try:\n","        # Implement Azure Storage upload logic here\n","        logging.info(\"✅ Uploaded FAISS index to Azure Storage\")\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to upload FAISS index: {str(e)}\")\n","        raise\n"],"metadata":{"id":"S45BhcOZ2q2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3️⃣ AI Search Indexing\n","credential = AzureKeyCredential(SEARCH_API_KEY)\n","index_client = SearchIndexClient(f\"https://{SEARCH_SERVICE_NAME}.search.windows.net\", credential)"],"metadata":{"id":"MlVIfn_v2xCH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4️⃣ Upload to Azure Blob Storage\n","blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n","container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n","\n","def upload_faiss_to_azure(local_directory):\n","    for file_name in os.listdir(local_directory):\n","        file_path = os.path.join(local_directory, file_name)\n","        if os.path.isfile(file_path):\n","            blob_client = container_client.get_blob_client(file_name)\n","            with open(file_path, \"rb\") as data:\n","                blob_client.upload_blob(data, overwrite=True)\n","            logging.info(f\"Uploaded: {file_name}\")\n"],"metadata":{"id":"s2a2VpkX2unU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4️⃣.5 sets up indexing of data\n","\n","def delete_search_index(index_name: str) -> None:\n","    try:\n","        # Check if index exists before attempting to delete\n","        try:\n","            index_client.get_index(index_name)\n","            index_client.delete_index(index_name)\n","            logging.info(f\"✅ Existing index '{index_name}' deleted successfully!\")\n","        except Exception:\n","            logging.info(f\"Index '{index_name}' does not exist\")\n","\n","    except Exception as e:\n","        logging.error(f\"Failed to delete index '{index_name}': {str(e)}\")\n","\n","\n","def create_search_index(force_recreate: bool = True):\n","    if force_recreate:\n","        delete_search_index(INDEX_NAME)\n","\n","    try:\n","        # Check if index already exists\n","        try:\n","            existing_index = index_client.get_index(INDEX_NAME)\n","            if not force_recreate:\n","                logging.info(f\"Index '{INDEX_NAME}' already exists, skipping creation.\")\n","                return\n","        except Exception:\n","            pass  # Index doesn't exist, proceed with creation\n","\n","        # Create algorithm configuration\n","        algorithm_config = HnswAlgorithmConfiguration(\n","            name=\"my-hnsw-config\",\n","            kind=\"hnsw\",\n","            parameters={\n","                \"m\": 4,\n","                \"efConstruction\": 400,\n","                \"efSearch\": 500,\n","                \"metric\": \"cosine\"\n","            }\n","        )\n","\n","        # Create vector search configuration\n","        vector_search_config = VectorSearch(\n","            algorithms=[algorithm_config],\n","            profiles=[\n","                VectorSearchProfile(\n","                    name=\"my-vector-profile\",\n","                    algorithm_configuration_name=\"my-hnsw-config\"\n","                )\n","            ]\n","        )\n","\n","        # Create index schema\n","        index_schema = SearchIndex(\n","            name=INDEX_NAME,\n","            fields=[\n","                SearchField(name=\"id\", type=SearchFieldDataType.String, key=True),\n","                SearchField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n","                SearchField(\n","                    name=\"content_vector\",\n","                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n","                    searchable=True,\n","                    vector_search_dimensions=1536,\n","                    vector_search_profile_name=\"my-vector-profile\"\n","                ),\n","            ],\n","            vector_search=vector_search_config\n","        )\n","\n","        # Create new index\n","        index_client.create_index(index_schema)\n","        logging.info(f\"✅ Index '{INDEX_NAME}' created successfully!\")\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to create index: {str(e)}\")\n","        raise\n","\n","\n","# FileSearchTool Class\n","class FileSearchTool:\n","    def __init__(self, vector_store_ids: List[str] | None = None):\n","        self.vector_store_ids = vector_store_ids or []\n","\n","    def add_vector_store(self, store_id: str) -> None:\n","        if store_id not in self.vector_store_ids:\n","            self.vector_store_ids.append(store_id)\n","\n","    def remove_vector_store(self, store_id: str) -> None:\n","        try:\n","            self.vector_store_ids.remove(store_id)\n","        except ValueError:\n","            logging.warning(f\"Store ID {store_id} not found in vector store list.\")\n","\n","    def execute(self, tool_call: Any) -> Any:\n","        logging.info(f\"Executing tool call: {tool_call}\")\n","        return None\n","\n","# Generate Vector Store\n","def generate_vector_store(store_id: str) -> None:\n","    file_search_tool = FileSearchTool()\n","    file_search_tool.add_vector_store(store_id)\n","    logging.info(f\"✅ Vector store '{store_id}' added successfully!\")\n","\n","# Execute Workflow\n","create_search_index(force_recreate=False)  # Set to False if you want to keep existing index\n","logging.info(\"✅ Search index creation workflow completed!\")"],"metadata":{"id":"Qc2VqZvsVQhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4️⃣.75 tests the depolyment\n","\n","def check_deployments():\n","    try:\n","        # Initialize the client\n","        client = AzureOpenAI(\n","            azure_endpoint=config_data[\"AZURE_OPENAI_ENDPOINT\"],\n","            api_key=config_data[\"AZURE_OPENAI_KEY\"],\n","            api_version=config_data[\"AZURE_OPENAI_APIVERSION\"]\n","        )\n","\n","        print(\"\\n=== Testing Deployment Access ===\")\n","        try:\n","            # Test a simple completion to verify deployment access\n","            response = client.chat.completions.create(\n","                model=config_data[\"DEPLOYMENT_NAME\"],  # Use deployment name here\n","                messages=[\n","                    {\"role\": \"user\", \"content\": \"Hello, is this working?\"}\n","                ],\n","                max_tokens=10\n","            )\n","            print(\"✅ Successfully connected to deployment\")\n","            print(f\"Response: {response.choices[0].message.content}\")\n","\n","        except Exception as e:\n","            print(f\"❌ Error accessing deployment: {str(e)}\")\n","            print(\"\\nTroubleshooting tips:\")\n","            print(\"1. Verify these match exactly with your Azure portal:\")\n","            print(f\"   - Deployment Name: {config_data['DEPLOYMENT_NAME']}\")\n","            print(f\"   - Endpoint: {config_data['AZURE_OPENAI_ENDPOINT']}\")\n","            print(\"2. Check if the API version is current\")\n","            print(\"3. Verify the API key has proper permissions\")\n","\n","    except Exception as e:\n","        print(f\"❌ Connection failed: {str(e)}\")\n","\n","check_deployments()"],"metadata":{"id":"T9_0Rfuq_mua"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the middle of this code block is where you want to change the assistant's name and prompt. At the bottom is where all the functions are called/runned, so make sure you match the files directory to the actual location\n"],"metadata":{"id":"GM_O6M4WvVOu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLR0jC0P194u"},"outputs":[],"source":["# 5️⃣ Retrieve and Associate Vector Store ID with Azure OpenAI Assistant\n","\n","\n","def associate_vector_store_with_ai(): #Associate vector store with Azure OpenAI Assistant.\n","    try:\n","        # Initialize Azure OpenAI client\n","        client = AzureOpenAI(\n","            azure_endpoint=config_data[\"AZURE_OPENAI_ENDPOINT\"],\n","            api_key=config_data[\"AZURE_OPENAI_KEY\"],\n","            api_version=config_data[\"AZURE_OPENAI_APIVERSION\"]\n","        )\n","\n","        # Create assistant with correct model\n","        try:\n","            vector_store = client.beta.vector_stores.create(name=\"NASDAQ documents\")\n","\n","            pdf_folder = \"/content/extracted_pdfs\"\n","            file_paths = [os.path.join(pdf_folder, file) for file in os.listdir(pdf_folder) if file.endswith(\".pdf\")]\n","\n","            file_streams = [open(path, \"rb\") for path in file_paths]\n","            file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n","                vector_store_id=vector_store.id, files=file_streams\n","            )\n","\n","            print(vector_store)\n","# This is where you name and create the prompt for the assistant\n","            assistant = client.beta.assistants.create(\n","                name=\"Financial data Assistant\",\n","                instructions=\"You are an Financial assistant that answers questions based on financial documents.\",\n","                model=config_data[\"DEPLOYMENT_NAME\"],\n","                tools=[{\"type\": \"file_search\"}],\n","                tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n","            )\n","\n","            logging.info(f\"✅ Assistant created successfully! ID: {assistant.id}\")\n","            return assistant.id\n","\n","        except Exception as e:\n","            logging.error(f\"❌ Failed to create assistant: {str(e)}\")\n","            return None\n","\n","    except Exception as e:\n","        logging.error(f\"❌ Failed to associate vector store: {str(e)}\")\n","        return None\n","\n","# Execute Workflow MAKE SURE THAT YOU USE THE RIGHT PATHS/FILENAMES\n","extract_zip(ZIP_FILE_PATH, \"/content/extracted_pdfs\")\n","doc_texts = extract_text_from_pdfs(\"/content/extracted_pdfs\")\n","chunked_texts = chunk_text(doc_texts)\n","create_faiss_index(chunked_texts)\n","upload_faiss_to_azure(FAISS_INDEX_PATH)\n","create_search_index(force_recreate=False)\n","print(associate_vector_store_with_ai())\n","\n","logging.info(\"✅ Full pipeline executed successfully!\")\n"]}]}